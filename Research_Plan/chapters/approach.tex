\section{Approach}
\label{sec:Approach}

\begin{figure}[h]
   \centering
    \includegraphics[width=1\textwidth]{images/farm-render-with-text.png}
    \caption{An example application of the results of this thesis. A UAV continuously surveys a field over the growing season and data analysis is delivered to farm operators for high-level decision making who may use existing farm equipment such as tractors or autonomous Unmanned Ground Vehicles (UGVs) for targeted intervention in the field}
    \label{fig:concept-overview}
\end{figure}

\textbf{The aim of this research is to develop the required technological capabilities which enable UAVs to operate autonomously and develop a useful and informative 3D representation and understanding within a farm environment.} This is divided into three main sub-tasks of \textit{state estimation}, \textit{environment modeling} and \textit{data interpretation}. The desired level of autonomy and user-friendliness demands further improvement of state of the art approaches and adaptations which take into consideration the environment under which the UAV is operating.


\subsection{State Estimation}
	
 The homogeneous visual nature of the crops on an agricultural field makes an accurate estimation of the UAV position difficult. Since GPS accuracy is not adequate for constructing the high resolution field maps or defining an optimal path for a UGV intervention, accurate localization of the UAV over the field will be obtained through a fusion of visual cues, inertial and GPS data. The current state of the art technique in visual inertial state estimation \cite{leutenegger2013keyframe} will be extended to incorporate GPS cues using an Extended Kalman Filter (EKF) with a focus on robustness during noisy visual-inertial estimates or absence of GPS data. Experiments to evaluate the performance of the state estimator on a real farm environment will be conducted using ground truth data from the Leica Geosystems Total Station.
	
\subsection{Environment Modeling}

 During this thesis, UAVs will be used to collect multi-spectral imagery and three-dimensional depth data, over farms, continuously over prolonged time periods (the growing season). The survey data collected by the on-board sensors over multiple flights will be used to build a multi-resolution spatio-temporal database of the field which encodes both three-dimensional geometry and changes over time.  The
  interpreted data will be used to generate maps in human readable
  format, with parameters suitable for informing the farmer, such as
  crop density, plant health, weed pressure and nitrogen nutrition
  status, as well as 3D key points for the UAV to use while navigating.

  This information will be used to support model-driven decision
  making for farm management. Over the short term, this may include
  determining a feasible path for a UGV through the field, detection
  of weeds, indicating areas requiring water or fertilizer and
  detecting any trespassing animals. Over the long term, the database
  may be used to measure crop growth over time and compare field
  performance over seasons to optimize when and how much external
  inputs are added, in order to maximize yield and minimize input.

  The aerial map construction process will address data association,
  filtering, data fusion, and map optimization.  The sensor data will
  be locally fused in a robust, multi-sensor, semi-direct optimization
  framework.  For each sensor, I will associate a confidence that
  depends on basic robot state parameters (e.g., the UAV altitude,
  time, etc.) and on the environment conditions (e.g., weather
  conditions, environment morphology, etc.).  The confidence
  information will be dynamically exploited inside the
  optimization. For instance, if the UAV is flying at a low altitude
  in a partially occluded environment, the GPS readings will be
  trusted less due to the possible signal occlusions or reflection
  resulting from the surrounding structures that decrease the quality
  of the GPS signal.



\subsection{Data Analysis and Interpretation}

 The raw data collected by on board sensors of the UAV will be analyzed and converted into semantic information about the crops growing on the field, and averaged over small parts ({\raise.17ex\hbox{$\scriptstyle\sim$}} 1 $m^2$). Specifically, the images captured by the RGBI and thermal cameras will be converted into normalized vegetation indices representing the general health of the crops on the field. Other well known plant growth and vitality indicators such as height, canopy cover and leaf greenness will also be estimated. These parameter estimates obtained using the UAV data, will be compared with state of the art estimates for the field parameters obtained from ETHZ's unique Field Phenotyping Platform (FIP) facility at Lindau-Eschikon in order to verify their accuracy and robustness. Furthermore, a scientific methodology for calculating farm input recommendations, using a combination of these parameters, will be developed and tested for efficacy, improving transparency amongst the general public and farmers who currently have to rely on black-box estimates provided by equipment manufacturers.


\iffalse


 


To achieve this goal, this thesis will target step changes in technological abilities in the following areas and showcase these improvements by applying these novel methods to important field management actions such as weed control.
\begin{denseItemize}
\item {\bf UAV perception for agriculture} -- multi-spectral mapping and robust estimation of spatial distribution of crop health and field status from aerial images.
\end{denseItemize}

This work package deals with the perception system of the UAV. In brief, this includes:
\begin{denseItemize}
\item A {\bf localization} module that determines the pose of the UAV within the field while surveying.
\item A module that will produce {\bf multi-resolution multi-spectral} aerial maps that encode the information needed to {\bf assess plant health}.
\item A module to {\bf interpret the aerial maps} and convert them into semantic information needed for {\bf agricultural decision making} both by the autonomous system, and by the farm operator.
\end{denseItemize}


\begin{deliverables}{\WPBNo}

\item {\bf UAV Localization} \putright{{\bf M15(i), M35}}
   \label{del:wpb:uavlocal}
   \delresponsible{\ETHZ}

  A UAV localization module which gives robust and accurate estimates of the UAV position and orientation on the field using a combination of the on-board sensor data and communication with the UGV.

\item {\bf UAV Environment Modeling} \putright{{\bf M25}}
   \label{del:wpb:mapcons}
   \delresponsible{\ETHZ}

   A software module for combining the gathered sensor data with the output of the localization module into time-stamped, geolocated maps which are dynamically updated with every UAV mission.% An initial version of this map encoding data collected over a single day will be delivered in M18. 
The final version 
%that can encode data collected over a season 
will be delivered in M25.

\item {\bf Data Analysis and Interpretation} \putright{{\bf M15(i), M35}}
   \label{del:wpb:dataanalysis}
   \delresponsible{\ETHZ}
   
   \begin{itemize}
   \item
   A software module for converting the raw data gathered by the UAV sensors into tested reliable vegetation indices and   plant growth and vitality indicators.
   \item
   A validation of the indices calculated using the UAV sensors with ground truth from the FIP.
   \item
   Correlations between vegetation indices, sensor data and farm inputs along with derivation methodology.
   \end{itemize}
   The initial version in M15 will work with the maps delivered in D\WPBNo.\ref{del:wpb:mapcons} at M15. The final version will work with the updated maps delivered in M35.

\end{deliverables}

\fi
%\begin{itemize}
%	\item \emph{Research tasks \& approach}
%	\item \emph{Expected results}
%\end{itemize}