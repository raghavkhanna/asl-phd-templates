\section{State of the Art}
\label{sec:State of the Art}

The use of low-altitude aerial remote sensing techniques such as unmanned aerial vehicles (UAV) is topic of many recent agricultural research programs addressing sensor usage, applications for breeding \cite{araus2014field}, farm management support systems \cite{primicerio2012flexible}, \cite{corcoles2013estimation} and environmental monitoring. Investigated crops range from field crops like wheat and maize to orchard crops like apple and vine to vegetables. Although different combinations of UAV, sensors, and information processing techniques are available, applications in practice are scarce.

To optimize the timing of management actions in the field information collection (survey) needs to be automated. UAV systems can collect information about the crops status in high temporal and spatial resolution, while alleviating the need for a ground vehicle to drive through the field, thus minimizing crop damage and soil compaction. To improve the applicability of such methods usability of UAV in the field needs to be simple or automated and information outputs need to be reliable and applicable for different farm management needs such as fertilization, weed control, pesticide management, and irrigation scheduling \cite{bongiovanni2004precision}. However, before this is possible, UAV perception requires two core competencies: (1) motion estimation and precise localization within the field, and (2) dense 3D situational awareness that can detect obstacles and bootstrap the 3D mapping process.

Visual localization and mapping for aerial robots is a very active research field, which has received great attention in the last few years. Ego-motion estimation using visual cues (commonly known as visual odometry) on aerial robots forms the backbone for their autonomy. Successful results for ground vehicles \cite{konolige2008outdoor,konolige2011large,comport2010real}, have paved the way to the recent exploitation of these principles into real-time systems for onboard operation on UAVs using visual information and/or laser scanner measurements. As UAVs are inherently unstable systems, the relatively low-bandwidth laser or visual measurements are augmented using complementary, high-frequency inertial measurements to enable highly dynamic flights using only onboard sensors \cite{shen2011autonomous,weiss2012real,weiss2012versatile,steder2008visual}. While these works combine these complementary sensing modality in a loosely coupled fashion, recent work showed that a tightly coupled optimization over inertial and visual error terms can lead to more robust and more accurate state estimates \cite{leutenegger_rss13,Li2013c}.

The sparse feature-based SLAM methods described above do not provide enough information for a robot to interact with its environment in 3D. 
Dense 3D information about local obstacles is required for autonomous path planning.  However, getting this information on a small low-powered multi-copters is still a challenge.  Recently, monocular dense visual odometry approaches demonstrated the feasibility to jointly estimate the camera pose as well as the full scene depth given the intensity values of the full images. The ``Kinect Fusion'' algorithm \cite{newcombe2011kinectfusion} pioneered high-quality real-time dense reconstruction using an RGBD camera, while \cite{newcombe2011dtam} demonstrated the first real-time dense reconstruction pipeline using a standard camera. However, the amount of computation required by these techniques is immense, necessitating the use of power-hungry GPUs and restricting the mode of operation to very small spaces. Similarly, \cite{sturm2013uavg} attempted to perform dense scene reconstruction from RGBD images captured from a quadrotor, while processing them on an off-board GPU. To reduce the power necessary for 3D scene perception, project members at ASL developed a lightweight low-power stereo sensor that contains a dedicated processor for 3D perception \cite{nikolic_icra14}. 
% Example references in parentheses format \citep{Raibert1986LeggedRobotsThatBalance, Vukobratovic2004ZeroMomentPoint} or as textual format as in \citet{Pratt1995SEA}.